from dotenv import load_dotenv
import pandas as pd
import json
import time
import os
from openai import OpenAI
from tqdm import tqdm  # è¿›åº¦æ¡åº“ï¼Œpip install tqdm


# ================= é…ç½®åŒº =================
load_dotenv()  # åŠ è½½ .env æ–‡ä»¶
# BASE_URL = "https://api.deepseek.com"  # æˆ–è€…æ™ºè°±çš„ url
#
# client = OpenAI(api_key=os.getenv("API_KEY"), base_url=BASE_URL)

client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.getenv("ARK_API_KEY"),
)

def ask_llm_expert(row):
    """
    å°†ä¸€è¡Œæ•°æ®è½¬åŒ–ä¸º Promptï¼Œå‘é€ç»™ LLMï¼Œè¿”å›å†³ç­–
    """
    # 1. æ„é€  Prompt
    # æŠ€å·§ï¼šæŠŠå…·ä½“æ•°å­—æ”¾å…¥ promptï¼Œå¹¶æ˜ç¡®è¦æ±‚ JSON æ ¼å¼
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªè¾¹ç¼˜è®¡ç®—è°ƒåº¦ä¸“å®¶ã€‚è¯·æ ¹æ®ä»»åŠ¡çŠ¶æ€å†³å®šæ˜¯å¦å¸è½½ä»»åŠ¡ã€‚"
        "å¦‚æœä»»åŠ¡å¤§ã€å¸¦å®½ä½ã€æœåŠ¡å™¨è´Ÿè½½é«˜ï¼Œå»ºè®®æœ¬åœ°å¤„ç†æˆ–è€…ä¸¢å¼ƒï¼ˆæ ¹æ®å…·ä½“ç­–ç•¥ï¼‰ã€‚"
        "å¦‚æœå¸¦å®½å……è¶³ä¸”æœåŠ¡å™¨ç©ºé—²ï¼Œå»ºè®®å¸è½½åˆ°è¾¹ç¼˜æœåŠ¡å™¨ã€‚"
        "è¯·åŠ¡å¿…è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š'action' (0ä»£è¡¨æœ¬åœ°å¤„ç†, 1ä»£è¡¨å¸è½½) å’Œ 'reason' (ç®€çŸ­ç†ç”±)ã€‚"
    )

    user_prompt = f"""
    å½“å‰çŠ¶æ€ï¼š
    - ä»»åŠ¡å¤§å°: {row['task_size']} MB
    - æ—¶å»¶è¦æ±‚: {row['latency_limit']} ms
    - å½“å‰å¸¦å®½: {row['bandwidth']} Mbps
    - æœåŠ¡å™¨è´Ÿè½½: {row['server_load']}%

    è¯·ç»™å‡ºå†³ç­–ã€‚
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-v3-2-251201",  # æˆ– glm-4
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            response_format={"type": "json_object"},  # å…³é”®ï¼å¼ºåˆ¶è¿”å› JSON
            temperature=0.1  # ä½éšæœºæ€§
        )

        # è§£æè¿”å›çš„å†…å®¹
        content = response.choices[0].message.content
        result_json = json.loads(content)  # æŠŠå­—ç¬¦ä¸²å˜æˆå­—å…¸

        return result_json['action'], result_json['reason']

    except Exception as e:
        print(f"API è¯·æ±‚å¤±è´¥: {e}")
        return None, "Error"


def main():
    # 1. è¯»å–åŸå§‹æ•°æ®
    try:
        df = pd.read_csv("raw_scenarios.csv")
    except FileNotFoundError:
        print("âŒ æ²¡æ‰¾åˆ° raw_scenarios.csvï¼Œè¯·å…ˆè¿è¡Œ Step 1 çš„ä»£ç ï¼")
        return

    print("ğŸš€ å¼€å§‹è¯·æ±‚ LLM è¿›è¡Œæ ‡æ³¨ï¼ˆTeacher æ¨¡å¼ï¼‰...")

    actions = []
    reasons = []

    # 2. å¾ªç¯å¤„ç†æ¯ä¸€è¡Œ (ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡)
    # df.iterrows() éå†æ¯ä¸€è¡Œæ•°æ®
    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        action, reason = ask_llm_expert(row)

        actions.append(action)
        reasons.append(reason)

        # âš ï¸ é‡è¦ï¼šåŠ ä¸Šå»¶æ—¶ï¼Œé˜²æ­¢è§¦å‘ API çš„é€Ÿç‡é™åˆ¶ (Rate Limit)
        # DeepSeek æ¯”è¾ƒå®½æ¾ï¼Œä½†å»ºè®®è¿˜æ˜¯åœ 0.5 ç§’
        time.sleep(0.5)

        # 3. ä¿å­˜ç»“æœ
    df['label_action'] = actions
    df['label_reason'] = reasons

    # å»é™¤å¤±è´¥çš„è¡Œ (None)
    df = df.dropna()

    output_file = "expert_data.csv"
    df.to_csv(output_file, index=False)
    print(f"\nâœ… æ ‡æ³¨å®Œæˆï¼é«˜è´¨é‡è®­ç»ƒæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    print(f"å…±è·å¾— {len(df)} æ¡æœ‰æ•ˆæ•°æ®ã€‚")
    print("-" * 20)
    print(df[['task_size', 'bandwidth', 'label_action', 'label_reason']].head())


if __name__ == "__main__":
    main()